{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 115439,
          "databundleVersionId": 13800781,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kavya-Jain5/Project1/blob/main/milestone_4_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone 4 ‚Äî Sequence Modeling with LSTM and GRU\n",
        "\n",
        "This milestone introduces **deep learning models (LSTM / GRU)** that are specifically designed to capture the **order and contextual relationships** between words in a sequence.\n",
        "\n",
        "---\n",
        "\n",
        "##  Suggested Readings\n",
        "- [LSTM](https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
        "- [GRU](https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Instructions\n",
        "\n",
        "Use the **constants and helper functions** provided in the next cell to answer all **Milestone-4 questions**.\n",
        "\n",
        "Perform the following tasks on the **training dataset** provided as part of the Kaggle competition:\n",
        "\n",
        "üîó **Competition Link:**  \n",
        "[2025-Sep-DL-Gen-AI-Project](https://www.kaggle.com/competitions/2025-sep-dl-gen-ai-project)\n"
      ],
      "metadata": {
        "id": "e2ogIMAMt4VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "naJr2EGft4VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:22:49.567144Z",
          "iopub.execute_input": "2025-10-24T17:22:49.567471Z",
          "iopub.status.idle": "2025-10-24T17:22:49.573706Z",
          "shell.execute_reply.started": "2025-10-24T17:22:49.567448Z",
          "shell.execute_reply": "2025-10-24T17:22:49.572570Z"
        },
        "id": "JbLUKraxt4VO"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set seeds and Constants"
      ],
      "metadata": {
        "id": "4J6MM3M4t4VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------- DON'T CHANGE THIS --------------------------\n",
        "DATA_SEED = 67\n",
        "TRAINING_SEED = 1234\n",
        "MAX_LEN = 50\n",
        "BATCH_SIZE = 64\n",
        "EMB_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 5\n",
        "\n",
        "random.seed(DATA_SEED)\n",
        "np.random.seed(DATA_SEED)\n",
        "torch.manual_seed(DATA_SEED)\n",
        "torch.cuda.manual_seed(DATA_SEED)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:24:38.648298Z",
          "iopub.execute_input": "2025-10-24T17:24:38.648610Z",
          "iopub.status.idle": "2025-10-24T17:24:38.656492Z",
          "shell.execute_reply.started": "2025-10-24T17:24:38.648588Z",
          "shell.execute_reply": "2025-10-24T17:24:38.655516Z"
        },
        "id": "FziGMCfXt4VO"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vocab"
      ],
      "metadata": {
        "id": "WdF4Ds3-t4VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path= '/content/train.csv'     # enter your data path here\n",
        "df = pd.read_csv(data_path)            # read it and store it in df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:37:54.421622Z",
          "iopub.execute_input": "2025-10-24T16:37:54.422106Z",
          "iopub.status.idle": "2025-10-24T16:37:54.474780Z",
          "shell.execute_reply.started": "2025-10-24T16:37:54.422083Z",
          "shell.execute_reply": "2025-10-24T16:37:54.473840Z"
        },
        "id": "SKh8SkoYt4VP"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91318978"
      },
      "source": [
        "# Split train df into train_df(80%) and test_df (20%) use seed\n",
        "# ------------------- write your code here -------------------------------\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=DATA_SEED)\n",
        "#-------------------------------------------------------------------------"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a simple space-based tokenizer.\n",
        "# ------------------- write your code here -------------------------------\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:44:17.602830Z",
          "iopub.execute_input": "2025-10-24T16:44:17.603704Z",
          "iopub.status.idle": "2025-10-24T16:44:17.608521Z",
          "shell.execute_reply.started": "2025-10-24T16:44:17.603674Z",
          "shell.execute_reply": "2025-10-24T16:44:17.607566Z"
        },
        "id": "HJdeRv_ht4VP"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99c5d231",
        "outputId": "451ebcf0-0827-4e9b-988c-4c5b0412a049"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "simple_lstm_model = SimpleLSTM(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
        "bilstm_model = BiLSTM(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
        "stacked_gru_model = StackedGRU(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, PAD_IDX)\n",
        "\n",
        "print(f\"Number of training parameters for Simple LSTM: {count_parameters(simple_lstm_model)}\")\n",
        "print(f\"Number of training parameters for Bidirectional LSTM: {count_parameters(bilstm_model)}\")\n",
        "print(f\"Number of training parameters for Stacked GRU: {count_parameters(stacked_gru_model)}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training parameters for Simple LSTM: 940877\n",
            "Number of training parameters for Bidirectional LSTM: 1308749\n",
            "Number of training parameters for Stacked GRU: 1243981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use counter to count all tokens in train_df\n",
        "token_counter = Counter()\n",
        "# ------------------- write your code here -------------------------------\n",
        "for text in train_df['text']:\n",
        "    token_counter.update(tokenize(text))\n",
        "#------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:44:45.867518Z",
          "iopub.execute_input": "2025-10-24T16:44:45.867829Z",
          "iopub.status.idle": "2025-10-24T16:44:45.902212Z",
          "shell.execute_reply.started": "2025-10-24T16:44:45.867807Z",
          "shell.execute_reply": "2025-10-24T16:44:45.900995Z"
        },
        "id": "kxjVnGSTt4VQ"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train and val dataloaders"
      ],
      "metadata": {
        "id": "BBR5zOYPt4VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------- DON'T CHANGE THIS --------------------------\n",
        "specials = ['<unk>', '<pad>']\n",
        "min_freq = 2\n",
        "vocab_list = specials + [token for token, freq in token_counter.items() if freq >= min_freq]\n",
        "word2idx = {token: i for i, token in enumerate(vocab_list)}\n",
        "UNK_IDX = word2idx['<unk>']\n",
        "PAD_IDX = word2idx['<pad>']\n",
        "def text_pipeline(text):\n",
        "    \"\"\"Converts text to a list of indices using the word2idx dict.\"\"\"\n",
        "    tokens = tokenize(text)\n",
        "    return [word2idx.get(token, UNK_IDX) for token in tokens]\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.texts = dataframe['text'].values\n",
        "        self.labels = dataframe[['anger', 'fear', 'joy', 'sadness', 'surprise']].values.astype(np.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_text, _labels) in batch:\n",
        "        label_list.append(_labels)\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)[:MAX_LEN]\n",
        "        text_list.append(processed_text)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=PAD_IDX)\n",
        "    if text_list.shape[1] < MAX_LEN:\n",
        "        pad_tensor = torch.full(\n",
        "            (text_list.shape[0], MAX_LEN - text_list.shape[1]),\n",
        "            PAD_IDX,\n",
        "            dtype=torch.int64\n",
        "        )\n",
        "        text_list = torch.cat((text_list, pad_tensor), dim=1)\n",
        "\n",
        "    return text_list, label_list\n",
        "\n",
        "# Create train and val dataloaders\n",
        "# ------------------- write your code here -------------------------------\n",
        "train_dataset = EmotionDataset(train_df)\n",
        "val_dataset = EmotionDataset(val_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "#------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:21:56.467351Z",
          "iopub.execute_input": "2025-10-24T17:21:56.467673Z",
          "iopub.status.idle": "2025-10-24T17:21:56.483356Z",
          "shell.execute_reply.started": "2025-10-24T17:21:56.467652Z",
          "shell.execute_reply": "2025-10-24T17:21:56.482391Z"
        },
        "id": "DTFAKSgPt4VQ"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Q0KEXh1wt4VQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What are the vocabulary size, padding token index, and unknown token index for the above dataset?"
      ],
      "metadata": {
        "id": "nwq9_wK4t4VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "VOCAB_SIZE = len(vocab_list)\n",
        "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
        "print(f\"Padding Token Index: {PAD_IDX}\")\n",
        "print(f\"Unknown Token Index: {UNK_IDX}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:57:14.541326Z",
          "iopub.execute_input": "2025-10-24T16:57:14.541611Z",
          "iopub.status.idle": "2025-10-24T16:57:14.546909Z",
          "shell.execute_reply.started": "2025-10-24T16:57:14.541593Z",
          "shell.execute_reply": "2025-10-24T16:57:14.546053Z"
        },
        "id": "InMGo-zwt4VQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56b6931-b069-4f63-e5c7-a2a9ad1224fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 5730\n",
            "Padding Token Index: 1\n",
            "Unknown Token Index: 0\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.What are the indices for the words \"happy\", \"alone\", and \"sad\" in the vocabulary?"
      ],
      "metadata": {
        "id": "W2s9owsCt4VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "print(f\"Index for 'happy': {word2idx.get('happy', UNK_IDX)}\")\n",
        "print(f\"Index for 'alone': {word2idx.get('alone', UNK_IDX)}\")\n",
        "print(f\"Index for 'sad': {word2idx.get('sad', UNK_IDX)}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:58:00.297702Z",
          "iopub.execute_input": "2025-10-24T16:58:00.298047Z",
          "iopub.status.idle": "2025-10-24T16:58:00.304387Z",
          "shell.execute_reply.started": "2025-10-24T16:58:00.298024Z",
          "shell.execute_reply": "2025-10-24T16:58:00.302684Z"
        },
        "id": "LVNCVQADt4VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d29cc7-b8d0-41c2-d284-d6d296714489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index for 'happy': 1578\n",
            "Index for 'alone': 2525\n",
            "Index for 'sad': 885\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:28:15.777599Z",
          "iopub.execute_input": "2025-10-24T17:28:15.777917Z",
          "iopub.status.idle": "2025-10-24T17:28:15.784144Z",
          "shell.execute_reply.started": "2025-10-24T17:28:15.777894Z",
          "shell.execute_reply": "2025-10-24T17:28:15.783195Z"
        },
        "id": "N83sTQ8At4VR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch to test shapes\n",
        "#take one batch as input here and store it in text_batch\n",
        "text_batch, _ = next(iter(train_dataloader))\n",
        "emb_layer = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
        "embedded_batch = emb_layer(text_batch)\n",
        "\n",
        "# Simple LSTM layer Output Shape (Use constants defined in 2nd cell)\n",
        "lstm = nn.LSTM(EMB_DIM, HIDDEN_DIM, batch_first=True)\n",
        "#read_output = lstm(embedded_batch)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:27.729908Z",
          "iopub.execute_input": "2025-10-24T17:43:27.730227Z",
          "iopub.status.idle": "2025-10-24T17:43:27.805156Z",
          "shell.execute_reply.started": "2025-10-24T17:43:27.730207Z",
          "shell.execute_reply": "2025-10-24T17:43:27.804231Z"
        },
        "id": "_6frZYdht4VR"
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. What is the output shape of the Embedding layer?\n"
      ],
      "metadata": {
        "id": "j2m_SwrZt4VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "print(embedded_batch.shape)\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:30.395104Z",
          "iopub.execute_input": "2025-10-24T17:43:30.395595Z",
          "iopub.status.idle": "2025-10-24T17:43:30.401656Z",
          "shell.execute_reply.started": "2025-10-24T17:43:30.395569Z",
          "shell.execute_reply": "2025-10-24T17:43:30.400766Z"
        },
        "id": "h4C8uR4zt4VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b2f07f-fd35-4567-babc-126e21b0b0e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 50, 100])\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. What will be output shape of simple LSTM layer"
      ],
      "metadata": {
        "id": "cQi8Xfhit4VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "lstm_output, _ = lstm(embedded_batch)\n",
        "print(lstm_output.shape)\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:33.022293Z",
          "iopub.execute_input": "2025-10-24T17:43:33.022614Z",
          "iopub.status.idle": "2025-10-24T17:43:33.027754Z",
          "shell.execute_reply.started": "2025-10-24T17:43:33.022577Z",
          "shell.execute_reply": "2025-10-24T17:43:33.026937Z"
        },
        "id": "eL299270t4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c56d8a3-e3b0-4bd5-9599-f5817e493b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 50, 256])\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. What is the 'hidden' state shape from a simple LSTM?"
      ],
      "metadata": {
        "id": "2WyMjs-Pt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "_, (hidden, cell) = lstm(embedded_batch)\n",
        "print(hidden.shape)\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:37.314103Z",
          "iopub.execute_input": "2025-10-24T17:43:37.314400Z",
          "iopub.status.idle": "2025-10-24T17:43:37.319394Z",
          "shell.execute_reply.started": "2025-10-24T17:43:37.314379Z",
          "shell.execute_reply": "2025-10-24T17:43:37.318208Z"
        },
        "id": "SFu7xJO2t4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a39cdde-d511-4bad-c14f-d1cd6acf9a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 256])\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. What is the 'hidden' state shape from a simple GRU?"
      ],
      "metadata": {
        "id": "P-_KR3gJt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# similarly do it for gru and find hidden state shape\n",
        "# ------------------- write your code here -------------------------------\n",
        "gru = nn.GRU(EMB_DIM, HIDDEN_DIM, batch_first=True)\n",
        "_, hidden_gru = gru(embedded_batch)\n",
        "print(hidden_gru.shape)\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:48:43.149832Z",
          "iopub.execute_input": "2025-10-24T17:48:43.150446Z",
          "iopub.status.idle": "2025-10-24T17:48:43.205538Z",
          "shell.execute_reply.started": "2025-10-24T17:48:43.150421Z",
          "shell.execute_reply": "2025-10-24T17:48:43.204760Z"
        },
        "id": "6zu5csBlt4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e35f283-7126-4da2-e5db-cfcd1a7a6698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 256])\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. What is the 'output' tensor shape from a bidirectional LSTM?"
      ],
      "metadata": {
        "id": "9B1Aw1S3t4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectional LSTM Output Shape\n",
        "# ------------------- write your code here -------------------------------\n",
        "bidirectional_lstm = nn.LSTM(EMB_DIM, HIDDEN_DIM, batch_first=True, bidirectional=True)\n",
        "output_bi_lstm, (hidden_bi_lstm, cell_bi_lstm) = bidirectional_lstm(embedded_batch)\n",
        "print(output_bi_lstm.shape)\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:48:51.968585Z",
          "iopub.execute_input": "2025-10-24T17:48:51.968881Z",
          "iopub.status.idle": "2025-10-24T17:48:52.117279Z",
          "shell.execute_reply.started": "2025-10-24T17:48:51.968860Z",
          "shell.execute_reply": "2025-10-24T17:48:52.116302Z"
        },
        "id": "DsDrU39At4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d61404f6-e069-4dda-9203-2dcc06085b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 50, 512])\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8. What is the 'hidden' state shape from a bidirectional LSTM?"
      ],
      "metadata": {
        "id": "3HoQngbQt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectional LSTM Hidden Shape\n",
        "# ------------------- write your code here -------------------------------\n",
        "print(hidden_bi_lstm.shape)\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:53:18.625946Z",
          "iopub.execute_input": "2025-10-24T17:53:18.626297Z",
          "iopub.status.idle": "2025-10-24T17:53:18.632909Z",
          "shell.execute_reply.started": "2025-10-24T17:53:18.626276Z",
          "shell.execute_reply": "2025-10-24T17:53:18.631609Z"
        },
        "id": "dRX_2qu3t4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9eb1ae-51fe-49ea-9b43-6cff3fea087f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 64, 256])\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9. Create 3 sequential models using the (Simple & Bidirectional)LSTM and Stacked GRU (2 layers)For all models, follow this(Embedding layer ‚Üí [LSTM / BiLSTM / Stacked GRU] ‚Üí Linear layer) architecture. What will be the training parameters in all 3 cases?(LSTM, BiLSTM, Stacked GRU)"
      ],
      "metadata": {
        "id": "wDxAlIkvt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T18:18:21.974713Z",
          "iopub.execute_input": "2025-10-24T18:18:21.975047Z",
          "iopub.status.idle": "2025-10-24T18:18:21.980302Z",
          "shell.execute_reply.started": "2025-10-24T18:18:21.975020Z",
          "shell.execute_reply": "2025-10-24T18:18:21.979147Z"
        },
        "id": "Y1xjyI8et4VT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "# Simple LSTM\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        hidden = hidden.squeeze(0)\n",
        "        return self.linear(hidden)\n",
        "\n",
        "# Bidirectional LSTM\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        return self.linear(hidden)\n",
        "\n",
        "# Stacked GRU\n",
        "class StackedGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim, pad_idx, n_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        output, hidden = self.gru(embedded)\n",
        "        hidden = hidden[-1,:,:].squeeze(0)\n",
        "        return self.linear(hidden)\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T18:21:28.488340Z",
          "iopub.execute_input": "2025-10-24T18:21:28.488678Z",
          "iopub.status.idle": "2025-10-24T18:21:28.530039Z",
          "shell.execute_reply.started": "2025-10-24T18:21:28.488657Z",
          "shell.execute_reply": "2025-10-24T18:21:28.529049Z"
        },
        "id": "jN6AU6wjt4VT"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10. If you experimented with both LSTM and GRU models using the same hyperparameters, which one achieved a better peak Macro F1-score in your W&B logs?"
      ],
      "metadata": {
        "id": "cM_78-Est4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "7JuXKBDLt4VT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q11. Compare the total training time for your best sequential model against the simple averaging model from Milestone 3. How much longer (in minutes or percentage) did the more complex model (LSTM and GRU) take to train for the same number of epochs?"
      ],
      "metadata": {
        "id": "rKXe8Bcht4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "DMt4dVRbt4VT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q12. If you experimented with both LSTM and GRU models using the same hyperparameters, which one achieved a better peak Macro F1-score in your W&B logs?"
      ],
      "metadata": {
        "id": "vmHxpiLst4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ExtGs52Ct4Vc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q13 Based on your experiments, what was the most impactful hyperparameter you tuned for your sequential model (e.g., learning rate, hidden size, number of layers, dropout rate)?"
      ],
      "metadata": {
        "id": "TA7mcOOKt4Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iog5kgTt65FI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}